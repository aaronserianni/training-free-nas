{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter notebook for running transformer metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configuration_electra import ElectraConfig\n",
    "from modeling_electra import ElectraModel\n",
    "from modeling_electra import ElectraLayer\n",
    "from transformers import ElectraTokenizerFast\n",
    "\n",
    "import csv\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = []\n",
    "with open(\"data/nas_benchmark.json\", 'r') as f:\n",
    "    configs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target dataset is openwebtex\n",
    "dataset = load_dataset(\"openwebtext\")\n",
    "tokenizer = ElectraTokenizerFast.from_pretrained(\"google/electra-small-discriminator\")\n",
    "\n",
    "def encode(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length')\n",
    "\n",
    "tokenized_dataset = dataset.map(encode, batched=True, num_proc=32)\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sample tokenized batch from dataset\n",
    "dataloader = torch.utils.data.DataLoader(dataset['train'], batch_size=128)\n",
    "inputs = tokenizer(next(iter(dataloader))['text'], truncation=True, padding='max_length', return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covariance calculations for Jacobian covariance and variations\n",
    "def covariance(jacobs):\n",
    "    jacob = torch.transpose(jacobs, 0, 1).reshape(jacobs.size(1), -1).cpu().numpy()\n",
    "    correlations = np.corrcoef(jacob)\n",
    "    v, _ = np.linalg.eig(correlations)\n",
    "    k = 1e-5\n",
    "    return -np.sum(np.log(v + k) + 1.0 / (v + k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine calculations for Jacobian cosine and variations\n",
    "def cosine(jacobs):\n",
    "    jacob = torch.transpose(jacobs, 0, 1).reshape(jacobs.size(1), -1).cpu().numpy()\n",
    "    norm = np.linalg.norm(jacob, axis=1)\n",
    "    normed = jacob / norm[:, None]\n",
    "    cosines = (-pairwise_distances(normed, metric=\"cosine\") + 1) - np.identity(\n",
    "        normed.shape[0]\n",
    "    )\n",
    "    summed = np.sum(np.power(np.absolute(cosines.flatten()), 1.0 / 20)) / 2\n",
    "    return 1 - (1 / (pow(cosines.shape[0], 2) - cosines.shape[0]) * summed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synaptic Diversity metric\n",
    "def synaptic_diversity(model):\n",
    "    metric_array = []\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, ElectraLayer):\n",
    "            for sublayer in layer.operation.modules():\n",
    "                if isinstance(sublayer, torch.nn.Linear):\n",
    "                    if (sublayer.weight is not None) and (\n",
    "                        sublayer.weight.grad is not None\n",
    "                    ):\n",
    "                        metric_array.append(\n",
    "                            torch.abs(\n",
    "                                torch.norm(sublayer.weight, \"nuc\")\n",
    "                                * torch.norm(sublayer.weight.grad, \"nuc\")\n",
    "                            )\n",
    "                        )\n",
    "    summed = torch.tensor(0.0)\n",
    "    for j in range(len(metric_array)):\n",
    "        summed += torch.nansum(metric_array[j])\n",
    "        \n",
    "    return summed.detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synaptic_diversity_normalized(model):\n",
    "    metric_array = []\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, ElectraLayer):\n",
    "            for sublayer in layer.operation.modules():\n",
    "                if isinstance(sublayer, torch.nn.Linear):\n",
    "                    if (sublayer.weight is not None) and (\n",
    "                        sublayer.weight.grad is not None\n",
    "                    ):\n",
    "                        metric_array.append(\n",
    "                            torch.abs(\n",
    "                                torch.norm(sublayer.weight, \"nuc\")\n",
    "                                * torch.norm(sublayer.weight.grad, \"nuc\")\n",
    "                            )\n",
    "                        )\n",
    "    \n",
    "    summed = torch.tensor(0.0)\n",
    "    for j in range(len(metric_array)):\n",
    "        summed += torch.nansum(metric_array[j])\n",
    "    summed /= len(metric_array)\n",
    "    \n",
    "    return summed.detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synaptic saliency metric\n",
    "def synaptic_saliency(model):\n",
    "    metric_array = []\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, ElectraLayer):\n",
    "            for sublayer in layer.intermediate.modules():\n",
    "                if isinstance(sublayer, torch.nn.Linear):\n",
    "                    metric_array.append(\n",
    "                        torch.abs(sublayer.weight * sublayer.weight.grad)\n",
    "                    )\n",
    "            for sublayer in layer.output.modules():\n",
    "                if isinstance(sublayer, torch.nn.Linear):\n",
    "                    metric_array.append(\n",
    "                        torch.abs(sublayer.weight * sublayer.weight.grad)\n",
    "                    )\n",
    "                    \n",
    "    summed = torch.tensor(0.0)\n",
    "    for j in range(len(metric_array)):\n",
    "        summed += torch.nansum(metric_array[j])\n",
    "        \n",
    "    return summed.detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synaptic_saliency_normalized(model):\n",
    "    metric_array = []\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, ElectraLayer):\n",
    "            for sublayer in layer.intermediate.modules():\n",
    "                if isinstance(sublayer, torch.nn.Linear):\n",
    "                    metric_array.append(\n",
    "                        torch.abs(sublayer.weight * sublayer.weight.grad)\n",
    "                    )\n",
    "            for sublayer in layer.output.modules():\n",
    "                if isinstance(sublayer, torch.nn.Linear):\n",
    "                    metric_array.append(\n",
    "                        torch.abs(sublayer.weight * sublayer.weight.grad)\n",
    "                    )\n",
    "                    \n",
    "    summed = torch.tensor(0.0)\n",
    "    for j in range(len(metric_array)):\n",
    "        summed += torch.nansum(metric_array[j])\n",
    "    summed /= len(metric_array)\n",
    "        \n",
    "    return summed.detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Distance metric\n",
    "def activation_distance(outputs):\n",
    "    metric_array = []\n",
    "    for output in outputs:\n",
    "        output = output[0].view(output.size(1), -1)\n",
    "        x = (output > 0).float()\n",
    "        K = x @ x.t()\n",
    "        K2 = (1.0 - x) @ (1.0 - x.t())\n",
    "        metric_array.append(K + K2)\n",
    "        \n",
    "    summed = torch.tensor(0.0)\n",
    "    for j in range(len(outputs)):\n",
    "        summed += torch.nansum(metric_array[j])\n",
    "    \n",
    "    return summed.detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_distance_normalized(outputs):\n",
    "    metric_array = []\n",
    "    for output in outputs:\n",
    "        output = output[0].view(output.size(1), -1)\n",
    "        x = (output > 0).float()\n",
    "        K = x @ x.t()\n",
    "        K2 = (1.0 - x) @ (1.0 - x.t())\n",
    "        metric_array.append(K + K2)\n",
    "        \n",
    "    summed = torch.tensor(0.0)\n",
    "    for j in range(len(outputs)):\n",
    "        summed += torch.nansum(metric_array[j])\n",
    "    summed /= len(metric_array)\n",
    "    \n",
    "    return summed.detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobian_score(model):\n",
    "    jacobs = model.embeddings.position_embeddings.weight.grad.detach()\n",
    "    return covariance(jacobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobian_score_cosine(model):\n",
    "    jacobs = model.embeddings.position_embeddings.weight.grad.detach()\n",
    "    return cosine(jacobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Head Importance metric\n",
    "def head_importance(model):\n",
    "    metric_array = []\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, ElectraLayer):\n",
    "            for sublayer in layer.operation.operation.modules():\n",
    "                if isinstance(sublayer, torch.nn.Linear):\n",
    "                    if (sublayer.weight is not None) and (\n",
    "                        sublayer.weight.grad is not None\n",
    "                    ) and sublayer.weight.shape[0] >= 128:\n",
    "                        metric_array.append(\n",
    "                            torch.abs(sublayer.weight.data * sublayer.weight.grad)\n",
    "                        )\n",
    "    summed = torch.tensor(0.0)\n",
    "    for j in range(len(metric_array)):\n",
    "        summed += torch.nansum(metric_array[j])\n",
    "        \n",
    "    return summed.detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head_importance_normalized(model):\n",
    "    metric_array = []\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, ElectraLayer):\n",
    "            for sublayer in layer.operation.operation.modules():\n",
    "                if isinstance(sublayer, torch.nn.Linear):\n",
    "                    if (sublayer.weight is not None) and (\n",
    "                        sublayer.weight.grad is not None\n",
    "                    ) and sublayer.weight.shape[0] >= 128:\n",
    "                        metric_array.append(\n",
    "                            torch.abs(sublayer.weight.data * sublayer.weight.grad)\n",
    "                        )\n",
    "    summed = torch.tensor(0.0)\n",
    "    for j in range(len(metric_array)):\n",
    "        summed += torch.nansum(metric_array[j])\n",
    "    summed /= len(metric_array)\n",
    "        \n",
    "    return summed.detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Confidence metric (for both head and softmax)\n",
    "def attention_condfidence(outputs):\n",
    "    metric_array = []\n",
    "    for output in outputs:\n",
    "        metric_array.append(torch.mean(torch.max(output, 1)[0]))\n",
    "    \n",
    "    summed = torch.tensor(0.0)\n",
    "    for j in range(len(outputs)):\n",
    "        summed += torch.nansum(metric_array[j])\n",
    "        \n",
    "    return summed.detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_condfidence_normalized(outputs):\n",
    "    metric_array = []\n",
    "    for output in outputs:\n",
    "        metric_array.append(torch.mean(torch.max(output, 1)[0]))\n",
    "    \n",
    "    summed = torch.tensor(0.0)\n",
    "    for j in range(len(metric_array)):\n",
    "        summed += torch.nansum(metric_array[j])\n",
    "    summed /= len(metric_array)\n",
    "    \n",
    "    return summed.detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run metrics on all model in benchmark\n",
    "with open(\"BERT_results_activation.csv\", \"a\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    header = [\"ID\",\n",
    "              \"GLUE Score\",\n",
    "              \"Synaptic Diversity\",\n",
    "              \"Synaptic Diversity Normalized\",\n",
    "              \"Synaptic Saliency\",\n",
    "              \"Synaptic Saliency Normalized\",\n",
    "              \"Activation Distance\",\n",
    "              \"Activation Distance Normalized\",\n",
    "              \"Jacobian Score\",\n",
    "              \"Jacobian Score Normalized\",\n",
    "              \"Number of Parameters\",\n",
    "              \"Head Importance\",\n",
    "              \"Head Importance Normalized\",\n",
    "              \"Head Confidence\",\n",
    "              \"Head Confidence Normalized\",\n",
    "              \"Head Softmax Confidence\",\n",
    "              \"Head Softmax Confidence Normalized\",\n",
    "             ]\n",
    "    writer.writerow(header)\n",
    "    f.flush()\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    for i in range(500):\n",
    "        np.random.seed(0)\n",
    "        torch.manual_seed(0)\n",
    "\n",
    "        nas_config = configs[i][\"hparams\"][\"model_hparam_overrides\"][\"nas_config\"]\n",
    "\n",
    "        config = ElectraConfig(\n",
    "            nas_config=nas_config, num_hidden_layers=len(nas_config[\"encoder_layers\"]), output_hidden_states=True\n",
    "        )\n",
    "        model = ElectraModel(config)\n",
    "        model.to(device)\n",
    "        inputs.to(device)\n",
    "        \n",
    "        # Hooks to get outputs at different layers\n",
    "        activation_outputs = []\n",
    "        def activation_hook(module, input, output):\n",
    "            activation_outputs.append(output)\n",
    "        for layer in model.modules():\n",
    "            if isinstance(layer, ElectraLayer):\n",
    "                sublayer = layer.intermediate.intermediate_act_fn.register_forward_hook(activation_hook)\n",
    "\n",
    "        head_outputs = []\n",
    "        def head_hook(module, input, output):\n",
    "            head_outputs.append(output)\n",
    "\n",
    "        # Initialize hooks\n",
    "        for layer in model.modules():\n",
    "            if isinstance(layer, ElectraLayer):\n",
    "                sublayer = layer.operation.operation\n",
    "                if hasattr(sublayer, 'query'):\n",
    "                    sublayer.query.register_forward_hook(head_hook)\n",
    "                if hasattr(sublayer, 'key'):\n",
    "                    sublayer.key.register_forward_hook(head_hook)\n",
    "                if hasattr(sublayer, 'value'):\n",
    "                    sublayer.value.register_forward_hook(head_hook)\n",
    "                if hasattr(sublayer, 'input'):\n",
    "                    sublayer.input.register_forward_hook(head_hook)\n",
    "                if hasattr(sublayer, 'weight'):\n",
    "                    sublayer.weight.register_forward_hook(head_hook)\n",
    "\n",
    "        softmax_outputs = []\n",
    "        def softmax_hook(module, input, output):\n",
    "            softmax_outputs.append(output)\n",
    "\n",
    "        for layer in model.modules():\n",
    "            if isinstance(layer, ElectraLayer):\n",
    "                sublayer = layer.operation.operation\n",
    "                if hasattr(sublayer, 'softmax'):\n",
    "                    sublayer.softmax.register_forward_hook(softmax_hook)\n",
    "\n",
    "        # Run gradient with respect to ones\n",
    "        model.zero_grad()\n",
    "        output = model(**inputs).last_hidden_state\n",
    "        output.backward(torch.ones_like(output))\n",
    "\n",
    "        row = [configs[i][\"id\"],\n",
    "               configs[i][\"scores\"][\"glue\"],\n",
    "               synaptic_diversity(model),\n",
    "               synaptic_diversity_normalized(model),\n",
    "               synaptic_saliency(model),\n",
    "               synaptic_saliency_normalized(model),\n",
    "               activation_distance(activation_outputs),\n",
    "               activation_distance_normalized(activation_outputs),\n",
    "               jacobian_score(model),\n",
    "               jacobian_score_cosine(model),\n",
    "               num_parameters(model),\n",
    "               head_importance(model),\n",
    "               head_importance_normalized(model),\n",
    "               attention_condfidence(head_outputs),\n",
    "               attention_condfidence_normalized(head_outputs),\n",
    "               attention_condfidence(softmax_outputs),\n",
    "               attention_condfidence_normalized(softmax_outputs),\n",
    "              ]\n",
    "        \n",
    "        writer.writerow(row)\n",
    "        f.flush()\n",
    "\n",
    "        print(str(configs[i][\"id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation_models = [285,\n",
    "116,\n",
    "280,\n",
    "337,\n",
    "464,\n",
    "166,\n",
    "153,\n",
    "157,\n",
    "330,\n",
    "164,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run metrics on ablation study of different minibatch inputs\n",
    "with open(\"BERT_batch_ablation.csv\", \"a\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    header = [\"ID\",\n",
    "              \"GLUE Score\",\n",
    "              \"Synaptic Diversity\",\n",
    "              \"Synaptic Diversity Normalized\",\n",
    "              \"Synaptic Saliency\",\n",
    "              \"Synaptic Saliency Normalized\",\n",
    "              \"Activation Distance\",\n",
    "              \"Activation Distance Normalized\",\n",
    "              \"Jacobian Score\",\n",
    "              \"Jacobian Score Normalized\",\n",
    "              \"Number of Parameters\",\n",
    "              \"Head Importance\",\n",
    "              \"Head Importance Normalized\",\n",
    "              \"Head Confidence\",\n",
    "              \"Head Confidence Normalized\",\n",
    "              \"Head Softmax Confidence\",\n",
    "              \"Head Softmax Confidence Normalized\",\n",
    "             ]\n",
    "    writer.writerow(header)\n",
    "    f.flush()\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    for i in ablation_models:\n",
    "        nas_config = configs[i][\"hparams\"][\"model_hparam_overrides\"][\"nas_config\"]\n",
    "\n",
    "        config = ElectraConfig(\n",
    "            nas_config=nas_config, num_hidden_layers=len(nas_config[\"encoder_layers\"]), output_hidden_states=True\n",
    "        )\n",
    "        model = ElectraModel(config)\n",
    "        model.to(device)\n",
    "        inputs.to(device)\n",
    "        \n",
    "        iterator = iter(dataloader)\n",
    "        \n",
    "        for j in range(10):\n",
    "            inputs = tokenizer(next(iterator)['text'], truncation=True, padding='max_length', return_tensors=\"pt\")\n",
    "            inputs.to(device)\n",
    "\n",
    "            np.random.seed(0)\n",
    "            torch.manual_seed(0)\n",
    "\n",
    "            activation_outputs = []\n",
    "            def activation_hook(module, input, output):\n",
    "                activation_outputs.append(output)\n",
    "            for layer in model.modules():\n",
    "                if isinstance(layer, ElectraLayer):\n",
    "                    sublayer = layer.intermediate.intermediate_act_fn.register_forward_hook(activation_hook)\n",
    "\n",
    "            head_outputs = []\n",
    "            def head_hook(module, input, output):\n",
    "                head_outputs.append(output)\n",
    "\n",
    "            for layer in model.modules():\n",
    "                if isinstance(layer, ElectraLayer):\n",
    "                    sublayer = layer.operation.operation\n",
    "                    if hasattr(sublayer, 'query'):\n",
    "                        sublayer.query.register_forward_hook(head_hook)\n",
    "                    if hasattr(sublayer, 'key'):\n",
    "                        sublayer.key.register_forward_hook(head_hook)\n",
    "                    if hasattr(sublayer, 'value'):\n",
    "                        sublayer.value.register_forward_hook(head_hook)\n",
    "                    if hasattr(sublayer, 'input'):\n",
    "                        sublayer.input.register_forward_hook(head_hook)\n",
    "                    if hasattr(sublayer, 'weight'):\n",
    "                        sublayer.weight.register_forward_hook(head_hook)\n",
    "\n",
    "            softmax_outputs = []\n",
    "            def softmax_hook(module, input, output):\n",
    "                softmax_outputs.append(output)\n",
    "\n",
    "            for layer in model.modules():\n",
    "                if isinstance(layer, ElectraLayer):\n",
    "                    sublayer = layer.operation.operation\n",
    "                    if hasattr(sublayer, 'softmax'):\n",
    "                        sublayer.softmax.register_forward_hook(softmax_hook)\n",
    "\n",
    "\n",
    "            model.zero_grad()\n",
    "            output = model(**inputs).last_hidden_state\n",
    "            output.backward(torch.ones_like(output))\n",
    "\n",
    "            row = [configs[i][\"id\"],\n",
    "                   configs[i][\"scores\"][\"glue\"],\n",
    "                   synaptic_diversity(model),\n",
    "                   synaptic_diversity_normalized(model),\n",
    "                   synaptic_saliency(model),\n",
    "                   synaptic_saliency_normalized(model),\n",
    "                   activation_distance(activation_outputs),\n",
    "                   activation_distance_normalized(activation_outputs),\n",
    "                   jacobian_score(model),\n",
    "                   jacobian_score_cosine(model),\n",
    "                   num_parameters(model),\n",
    "                   head_importance(model),\n",
    "                   head_importance_normalized(model),\n",
    "                   attention_condfidence(head_outputs),\n",
    "                   attention_condfidence_normalized(head_outputs),\n",
    "                   attention_condfidence(softmax_outputs),\n",
    "                   attention_condfidence_normalized(softmax_outputs),\n",
    "                  ]\n",
    "\n",
    "            writer.writerow(row)\n",
    "            f.flush()\n",
    "\n",
    "            print(str(configs[i][\"id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run metrics on ablation study of different initialization states\n",
    "with open(\"BERT_initialization_ablation.csv\", \"a\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    header = [\"ID\",\n",
    "              \"GLUE Score\",\n",
    "              \"Synaptic Diversity\",\n",
    "              \"Synaptic Diversity Normalized\",\n",
    "              \"Synaptic Saliency\",\n",
    "              \"Synaptic Saliency Normalized\",\n",
    "              \"Activation Distance\",\n",
    "              \"Activation Distance Normalized\",\n",
    "              \"Jacobian Score\",\n",
    "              \"Jacobian Score Normalized\",\n",
    "              \"Number of Parameters\",\n",
    "              \"Head Importance\",\n",
    "              \"Head Importance Normalized\",\n",
    "              \"Head Confidence\",\n",
    "              \"Head Confidence Normalized\",\n",
    "              \"Head Softmax Confidence\",\n",
    "              \"Head Softmax Confidence Normalized\",\n",
    "             ]\n",
    "    writer.writerow(header)\n",
    "    f.flush()\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    for i in ablation_models:\n",
    "        nas_config = configs[i][\"hparams\"][\"model_hparam_overrides\"][\"nas_config\"]\n",
    "\n",
    "        config = ElectraConfig(\n",
    "            nas_config=nas_config, num_hidden_layers=len(nas_config[\"encoder_layers\"]), output_hidden_states=True\n",
    "        )\n",
    "        model = ElectraModel(config)\n",
    "        model.to(device)\n",
    "        inputs.to(device)\n",
    "        \n",
    "        for j in range(10):\n",
    "            np.random.seed(j)\n",
    "            torch.manual_seed(j)\n",
    "\n",
    "            activation_outputs = []\n",
    "            def activation_hook(module, input, output):\n",
    "                activation_outputs.append(output)\n",
    "            for layer in model.modules():\n",
    "                if isinstance(layer, ElectraLayer):\n",
    "                    sublayer = layer.intermediate.intermediate_act_fn.register_forward_hook(activation_hook)\n",
    "\n",
    "            head_outputs = []\n",
    "            def head_hook(module, input, output):\n",
    "                head_outputs.append(output)\n",
    "\n",
    "            for layer in model.modules():\n",
    "                if isinstance(layer, ElectraLayer):\n",
    "                    sublayer = layer.operation.operation\n",
    "                    if hasattr(sublayer, 'query'):\n",
    "                        sublayer.query.register_forward_hook(head_hook)\n",
    "                    if hasattr(sublayer, 'key'):\n",
    "                        sublayer.key.register_forward_hook(head_hook)\n",
    "                    if hasattr(sublayer, 'value'):\n",
    "                        sublayer.value.register_forward_hook(head_hook)\n",
    "                    if hasattr(sublayer, 'input'):\n",
    "                        sublayer.input.register_forward_hook(head_hook)\n",
    "                    if hasattr(sublayer, 'weight'):\n",
    "                        sublayer.weight.register_forward_hook(head_hook)\n",
    "\n",
    "            softmax_outputs = []\n",
    "            def softmax_hook(module, input, output):\n",
    "                softmax_outputs.append(output)\n",
    "\n",
    "            for layer in model.modules():\n",
    "                if isinstance(layer, ElectraLayer):\n",
    "                    sublayer = layer.operation.operation\n",
    "                    if hasattr(sublayer, 'softmax'):\n",
    "                        sublayer.softmax.register_forward_hook(softmax_hook)\n",
    "\n",
    "\n",
    "            model.zero_grad()\n",
    "            output = model(**inputs).last_hidden_state\n",
    "            output.backward(torch.ones_like(output))\n",
    "\n",
    "            row = [configs[i][\"id\"],\n",
    "                   configs[i][\"scores\"][\"glue\"],\n",
    "                   synaptic_diversity(model),\n",
    "                   synaptic_diversity_normalized(model),\n",
    "                   synaptic_saliency(model),\n",
    "                   synaptic_saliency_normalized(model),\n",
    "                   activation_distance(activation_outputs),\n",
    "                   activation_distance_normalized(activation_outputs),\n",
    "                   jacobian_score(model),\n",
    "                   jacobian_score_cosine(model),\n",
    "                   num_parameters(model),\n",
    "                   head_importance(model),\n",
    "                   head_importance_normalized(model),\n",
    "                   attention_condfidence(head_outputs),\n",
    "                   attention_condfidence_normalized(head_outputs),\n",
    "                   attention_condfidence(softmax_outputs),\n",
    "                   attention_condfidence_normalized(softmax_outputs),\n",
    "                  ]\n",
    "\n",
    "            writer.writerow(row)\n",
    "            f.flush()\n",
    "\n",
    "            print(str(configs[i][\"id\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
