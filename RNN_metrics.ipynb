{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "339bec9e",
   "metadata": {},
   "source": [
    "## Jupyter notebook for running transformer metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8810f2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install networkx sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a118a89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/fmsnew/nas-bench-nlp-release.git\n",
    "%cd nas-bench-nlp-release\n",
    "!unzip -nq data/datasets.zip -d data/\n",
    "!unzip -nq train_logs_multi_runs/logs.zip -d train_logs_multi_runs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36291590",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from argparse import Namespace\n",
    "\n",
    "import data as nas_data\n",
    "import numpy as np\n",
    "import torch\n",
    "from model import AWDRNNModel\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from splitcross import SplitCrossEntropyLoss\n",
    "from train import evaluate, train\n",
    "from utils import batchify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584755f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_activations(inp):\n",
    "    try:\n",
    "        if isinstance(inp, tuple):\n",
    "            inp = inp[0]\n",
    "        # reshape input tensor to be batch size x single dimensional\n",
    "        inp = inp[0].view(inp.size(1), -1)\n",
    "        # will ReLU unit be active or not for each input? (binary codes) store in new tensor\n",
    "        x = (inp > 0).float()\n",
    "        # calculations for hamming distance\n",
    "        K = x @ x.t()\n",
    "        K2 = (1.0 - x) @ (1.0 - x.t())\n",
    "        # sum above to rest of calculations, store as numpy array in cpu memory\n",
    "        global K_score\n",
    "        K_score = K_score + K.cpu().numpy() + K2.cpu().numpy()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa6ff19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn\n",
    "from multilinear import MultiLinear\n",
    "\n",
    "# From NAS-Bench-NLP https://github.com/fmsnew/nas-bench-nlp-release\n",
    "class CustomRNNCell(torch.nn.Module):\n",
    "    elementwise_ops_dict = {\"prod\": torch.mul, \"sum\": torch.add}\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, recepie):\n",
    "        super(CustomRNNCell, self).__init__()\n",
    "\n",
    "        self.activations_dict = {\n",
    "            \"tanh\": torch.nn.Tanh(),\n",
    "            \"sigm\": torch.nn.Sigmoid(),\n",
    "            \"leaky_relu\": torch.nn.LeakyReLU(),\n",
    "        }\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.recepie = recepie\n",
    "        self.hidden_tuple_size = 0\n",
    "\n",
    "        components_dict = {}\n",
    "\n",
    "        self.G = nx.DiGraph()\n",
    "        for k in recepie.keys():\n",
    "            if k not in components_dict:\n",
    "\n",
    "                component = self._make_component(recepie[k])\n",
    "                if component is not None:\n",
    "                    components_dict[k] = component\n",
    "                if k.startswith(\"h_new\"):\n",
    "                    suffix = k.replace(\"h_new_\", \"\")\n",
    "                    if suffix.isdigit():\n",
    "                        self.hidden_tuple_size = max(\n",
    "                            [self.hidden_tuple_size, int(suffix) + 1]\n",
    "                        )\n",
    "\n",
    "                if k not in self.G.nodes():\n",
    "                    self.G.add_node(k)\n",
    "                for i, n in enumerate(recepie[k][\"input\"]):\n",
    "                    if n not in self.G.nodes():\n",
    "                        self.G.add_node(k)\n",
    "                    self.G.add_edge(n, k)\n",
    "\n",
    "        self.components = torch.nn.ModuleDict(components_dict)\n",
    "        self.nodes_order = list(nx.algorithms.dag.topological_sort(self.G))\n",
    "\n",
    "    def forward(self, x, hidden_tuple):\n",
    "        calculated_nodes = {}\n",
    "        # Modified to be able to get hidden states\n",
    "        hidden_tuple[0].requires_grad_()\n",
    "        hidden_tuple[0].retain_grad()\n",
    "        hidden_states.append(hidden_tuple[0])\n",
    "        for n in self.nodes_order:\n",
    "            if n == \"x\":\n",
    "                calculated_nodes[\"x\"] = x.unsqueeze(0)\n",
    "            elif n.startswith(\"h_prev\") and n.replace(\"h_prev_\", \"\").isdigit():\n",
    "                calculated_nodes[n] = hidden_tuple[\n",
    "                    int(n.replace(\"h_prev_\", \"\"))\n",
    "                ].unsqueeze(0)\n",
    "            elif n in self.components:\n",
    "                inputs = [calculated_nodes[k] for k in self.recepie[n][\"input\"]]\n",
    "                calculated_nodes[n] = self.components[n](*inputs)\n",
    "            else:\n",
    "                # simple operations\n",
    "                op = self.recepie[n][\"op\"]\n",
    "                inputs = [calculated_nodes[k] for k in self.recepie[n][\"input\"]]\n",
    "                if op in [\"elementwise_prod\", \"elementwise_sum\"]:\n",
    "                    op_func = CustomRNNCell.elementwise_ops_dict[\n",
    "                        op.replace(\"elementwise_\", \"\")\n",
    "                    ]\n",
    "                    calculated_nodes[n] = op_func(inputs[0], inputs[1])\n",
    "                    for inp in range(2, len(inputs)):\n",
    "                        calculated_nodes[n] = op_func(calculated_nodes[n], inputs[i])\n",
    "                elif op == \"blend\":\n",
    "                    calculated_nodes[n] = (\n",
    "                        inputs[0] * inputs[1] + (1 - inputs[0]) * inputs[2]\n",
    "                    )\n",
    "                elif op.startswith(\"activation\"):\n",
    "                    op_func = self.activations_dict[op.replace(\"activation_\", \"\")]\n",
    "                    calculated_nodes[n] = op_func(inputs[0])\n",
    "                    # calculate and store K codes for activations in RNN - LeakyReLU, TanH, Sigmoid\n",
    "                    calculate_activations(calculated_nodes[n])\n",
    "        return tuple(\n",
    "            [calculated_nodes[f\"h_new_{i}\"][0] for i in range(self.hidden_tuple_size)]\n",
    "        )\n",
    "\n",
    "    def _make_component(self, spec):\n",
    "        if spec[\"op\"] == \"linear\":\n",
    "            input_sizes = [\n",
    "                self.input_size if inp == \"x\" else self.hidden_size\n",
    "                for inp in spec[\"input\"]\n",
    "            ]\n",
    "            return MultiLinear(input_sizes, self.hidden_size)\n",
    "\n",
    "\n",
    "class CustomRNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, recepie):\n",
    "        super(CustomRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell = CustomRNNCell(input_size, hidden_size, recepie)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def forward(self, inputs, hidden_tuple=None):\n",
    "        batch_size = inputs.size(1)\n",
    "        if hidden_tuple is None:\n",
    "            hidden_tuple = tuple(\n",
    "                [\n",
    "                    self.init_hidden(batch_size)\n",
    "                    for _ in range(self.cell.hidden_tuple_size)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        self.check_hidden_size(hidden_tuple, batch_size)\n",
    "\n",
    "        hidden_tuple = tuple([x[0] for x in hidden_tuple])\n",
    "        outputs = []\n",
    "        for x in torch.unbind(inputs, dim=0):\n",
    "            hidden_tuple = self.cell(x, hidden_tuple)\n",
    "            outputs.append(hidden_tuple[0].clone())\n",
    "\n",
    "        return torch.stack(outputs, dim=0), tuple(\n",
    "            [x.unsqueeze(0) for x in hidden_tuple]\n",
    "        )\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # num_layers == const (1)\n",
    "        return torch.zeros(1, batch_size, self.hidden_size).to(\n",
    "            next(self.parameters()).device\n",
    "        )\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for param in self.parameters():\n",
    "            torch.nn.init.uniform_(param, -stdv, stdv)\n",
    "\n",
    "    def check_hidden_size(self, hidden_tuple, batch_size):\n",
    "        expected_hidden_size = (1, batch_size, self.hidden_size)\n",
    "        msg = \"Expected hidden size {}, got {}\"\n",
    "        for hx in hidden_tuple:\n",
    "            if hx.size() != expected_hidden_size:\n",
    "                raise RuntimeError(msg.format(expected_hidden_size, tuple(hx.size())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19316c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn\n",
    "from embed_regularize import embedded_dropout\n",
    "from locked_dropout import LockedDropout\n",
    "from weight_drop import ParameterListWeightDrop, WeightDrop\n",
    "\n",
    "# From NAS-Bench-NLP https://github.com/fmsnew/nas-bench-nlp-release\n",
    "class AWDRNNModel(torch.nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    # add batch_size parameter\n",
    "    def __init__(\n",
    "        self,\n",
    "        rnn_type,\n",
    "        ntoken,\n",
    "        ninp,\n",
    "        nhid,\n",
    "        nlayers,\n",
    "        dropout=0.5,\n",
    "        dropouth=0.5,\n",
    "        dropouti=0.5,\n",
    "        dropoute=0.1,\n",
    "        wdrop=0,\n",
    "        tie_weights=False,\n",
    "        recepie=None,\n",
    "        verbose=True,\n",
    "    ):\n",
    "        super(AWDRNNModel, self).__init__()\n",
    "        self.lockdrop = LockedDropout()\n",
    "        self.idrop = torch.nn.Dropout(dropouti)\n",
    "        self.hdrop = torch.nn.Dropout(dropouth)\n",
    "        self.drop = torch.nn.Dropout(dropout)\n",
    "        self.encoder = torch.nn.Embedding(ntoken, ninp)\n",
    "        self.wdrop = wdrop\n",
    "        self.verbose = verbose\n",
    "        self.ntoken = ntoken\n",
    "\n",
    "        if recepie is not None:\n",
    "            recepie = json.loads(recepie)\n",
    "\n",
    "        self.rnns = []\n",
    "        for i in range(nlayers):\n",
    "            input_size = ninp if i == 0 else nhid\n",
    "            hidden_size = nhid if i != nlayers - 1 else (ninp if tie_weights else nhid)\n",
    "            if rnn_type == \"LSTM\":\n",
    "                self.rnns.append(torch.nn.LSTM(input_size, hidden_size))\n",
    "            elif rnn_type == \"CustomRNN\":\n",
    "                self.rnns.append(CustomRNN(input_size, hidden_size, recepie))\n",
    "\n",
    "        if wdrop:\n",
    "            if rnn_type == \"LSTM\":\n",
    "                self.rnns = [\n",
    "                    WeightDrop(rnn, [\"weight_hh_l0\"], dropout=wdrop)\n",
    "                    for rnn in self.rnns\n",
    "                ]\n",
    "            elif rnn_type == \"CustomRNN\":\n",
    "                wd_rnns = []\n",
    "                for rnn in self.rnns:\n",
    "                    multilinear_components = []\n",
    "                    for k, v in rnn.cell.components.items():\n",
    "                        if rnn.cell.recepie[k][\"op\"] == \"linear\":\n",
    "                            for i in np.where(\n",
    "                                np.array(rnn.cell.recepie[k][\"input\"]) != \"x\"\n",
    "                            )[0]:\n",
    "                                multilinear_components.append(\n",
    "                                    f\"cell.components.{k}.weights.{i}\"\n",
    "                                )\n",
    "                    wd_rnns.append(\n",
    "                        ParameterListWeightDrop(\n",
    "                            rnn, multilinear_components, dropout=wdrop\n",
    "                        )\n",
    "                    )\n",
    "                    self.rnns = wd_rnns\n",
    "\n",
    "        if self.verbose:\n",
    "            print(self.rnns)\n",
    "        self.rnns = torch.nn.ModuleList(self.rnns)\n",
    "        self.decoder = torch.nn.Linear(nhid, ntoken)\n",
    "\n",
    "        if tie_weights:\n",
    "            self.decoder.weight = self.encoder.weight\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.ninp = ninp\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "        self.dropout = dropout\n",
    "        self.dropouti = dropouti\n",
    "        self.dropouth = dropouth\n",
    "        self.dropoute = dropoute\n",
    "        self.tie_weights = tie_weights\n",
    "        self.recepie = recepie\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, input, hidden, return_h=False, skip_embedding=False):\n",
    "\n",
    "        emb = (\n",
    "            input\n",
    "            if skip_embedding\n",
    "            else embedded_dropout(\n",
    "                self.encoder, input, dropout=self.dropoute if self.training else 0\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # store embedding output\n",
    "        self.embeddings = emb\n",
    "        # emb = self.idrop(emb)\n",
    "\n",
    "        emb = self.lockdrop(emb, self.dropouti)\n",
    "\n",
    "        raw_output = emb\n",
    "        new_hidden = []\n",
    "        raw_outputs = []\n",
    "        outputs = []\n",
    "        for i, rnn in enumerate(self.rnns):\n",
    "            raw_output, new_h = rnn(raw_output, hidden[i])\n",
    "            new_hidden.append(new_h)\n",
    "            raw_outputs.append(raw_output)\n",
    "            if i != self.nlayers - 1:\n",
    "                # self.hdrop(raw_output) add???\n",
    "                raw_output = self.lockdrop(raw_output, self.dropouth)\n",
    "                outputs.append(raw_output)\n",
    "        hidden = new_hidden\n",
    "\n",
    "        output = self.lockdrop(raw_output, self.dropout)\n",
    "        outputs.append(output)\n",
    "        result = output.view(output.size(0) * output.size(1), output.size(2))\n",
    "        if return_h:\n",
    "            return result, hidden, raw_outputs, outputs\n",
    "        return result, hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = []\n",
    "        for i in range(self.nlayers):\n",
    "            if self.rnn_type == \"LSTM\":\n",
    "                hidden_tuple_size = 2\n",
    "            elif self.rnn_type == \"CustomRNN\":\n",
    "                if self.wdrop:\n",
    "                    # wrapped with ParameterListWeightDrop\n",
    "                    hidden_tuple_size = self.rnns[0].module.cell.hidden_tuple_size\n",
    "                else:\n",
    "                    hidden_tuple_size = self.rnns[0].cell.hidden_tuple_size\n",
    "            hidden_size = (\n",
    "                self.nhid\n",
    "                if i != self.nlayers - 1\n",
    "                else (self.ninp if self.tie_weights else self.nhid)\n",
    "            )\n",
    "            hidden.append(\n",
    "                tuple(\n",
    "                    [\n",
    "                        weight.new(1, bsz, hidden_size).zero_()\n",
    "                        for _ in range(hidden_tuple_size)\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dda8d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source, i, args, seq_len=None, evaluation=False):\n",
    "    seq_len = min(seq_len if seq_len else args.bptt, len(source) - 1 - i)\n",
    "    data_ = source[i : i + seq_len]\n",
    "    target = source[i + 1 : i + 1 + seq_len].view(-1)\n",
    "    return data_, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0273c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors,\n",
    "    to detach them from their history.\"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781afe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Jacobian of layer\n",
    "def get_batch_jacobian(net, x, target, device, hidden, args=None, skip_embedding=False):\n",
    "    # reset gradient of network\n",
    "    net.zero_grad()\n",
    "    # begin recording all operations on input\n",
    "    # run network on input batch with hooks, returns classified and raw output\n",
    "    y, out = net(x, hidden, skip_embedding=skip_embedding)\n",
    "    net.embeddings.retain_grad()\n",
    "    # backpropogate from output with regards to gradients of 1 (used for old jacobian metric)\n",
    "    y.backward(torch.ones_like(y))\n",
    "    # get gradients of inputs\n",
    "    jacob = net.embeddings.grad.detach()\n",
    "    outputs = []\n",
    "    for output in out:\n",
    "        outputs.append(output[0].detach())\n",
    "    return jacob, target.detach(), y.detach(), outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebcd860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hooklogdet(K, labels=None):\n",
    "    # compute natural logarithm of determinant of array (the final score)\n",
    "    s, ld = np.linalg.slogdet(K)\n",
    "    return ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2c3bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jacobian Cosine Score\n",
    "def jacobian_score(jacobs):\n",
    "    jacob = torch.transpose(jacobs, 0, 1).reshape(jacobs.size(1), -1).cpu().numpy()\n",
    "    # calculate Pearson product-moment correlation coefficients\n",
    "    correlations = np.corrcoef(jacob)\n",
    "    # compute eignenvalues of matrix, discard normalized eigenvectors\n",
    "    v, _ = np.linalg.eig(correlations)\n",
    "    # compute final score\n",
    "    k = 1e-5\n",
    "    return -np.sum(np.log(v + k) + 1.0 / (v + k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdfe601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jacobian Cosine Score\n",
    "def cosine_score(jacobs):\n",
    "    jacob = torch.transpose(jacobs, 0, 1).reshape(jacobs.size(1), -1).cpu().numpy()\n",
    "    norm = np.linalg.norm(jacob, axis=1)\n",
    "    normed = jacob / norm[:, None]\n",
    "    cosines = (-pairwise_distances(normed, metric=\"cosine\") + 1) - np.identity(\n",
    "        normed.shape[0]\n",
    "    )\n",
    "    summed = np.sum(np.power(np.absolute(cosines.flatten()), 1.0 / 20)) / 2\n",
    "    return 1 - (1 / (pow(cosines.shape[0], 2) - cosines.shape[0]) * summed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de81c1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jacobian Noise Score\n",
    "def noised_jacobian(\n",
    "    network, amplitude, embedding_output, variance, noise, target, device, hidden, args\n",
    "):\n",
    "    noisy = embedding_output + amplitude * variance * noise\n",
    "    noisy = torch.tensor(noisy, requires_grad=True).float().to(device)\n",
    "    noise_jacobs, labels, y, out = get_batch_jacobian(\n",
    "        network, noisy, target, device, hidden, args, skip_embedding=True\n",
    "    )\n",
    "    return noise_jacobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bece2116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synflow (Synaptic Saliency) Score\n",
    "def synflow(net, x, target, device, hidden, args=None, skip_embedding=False):\n",
    "    net.zero_grad()\n",
    "    y, out = net(x, hidden, skip_embedding=False)\n",
    "    y.backward(y)\n",
    "\n",
    "    metric_array = []\n",
    "    for layer in net.modules():\n",
    "        if isinstance(layer, MultiLinear):\n",
    "            for i in range(1, len(layer.weights)):\n",
    "                if layer.weights[i].grad is not None:\n",
    "                    metric_array.append(\n",
    "                        torch.abs(layer.weights[i] * layer.weights[i].grad)\n",
    "                    )\n",
    "                else:\n",
    "                    metric_array.append(torch.zeros_like(layer.weights[i]))\n",
    "        if isinstance(layer, torch.nn.Linear):\n",
    "\n",
    "            if layer.weight is not None:\n",
    "                metric_array.append(\n",
    "                    torch.abs(layer.weight.double() * layer.weight.grad.double())\n",
    "                )\n",
    "            else:\n",
    "                metric_array.append(torch.zeros_like(layer.weight))\n",
    "    \n",
    "    sum = 0.0\n",
    "    for i in range(len(metric_array)):\n",
    "        sum += torch.nansum(metric_array[i])\n",
    "    return sum.detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef931ff7-bab5-4d60-8cb4-447b6580c28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden state scores\n",
    "def hidden_scores(hiddens):\n",
    "    metric_array = []\n",
    "    for hidden in hidden_states:\n",
    "        if hidden.grad is not None:\n",
    "            metric_array.append(torch.abs(hidden * hidden.grad))\n",
    "        else:\n",
    "            metric_array.append(torch.zeros_like(hidden))\n",
    "    sum = 0.0\n",
    "    for i in range(len(metric_array)):\n",
    "        sum += torch.nansum(metric_array[i])\n",
    "\n",
    "    hidden_state1 = torch.dstack([i.detach() for i in hidden_states[:70]])\n",
    "    hidden_state2 = torch.dstack([i.detach() for i in hidden_states[70:140]])\n",
    "    hidden_state3 = torch.dstack([i.detach() for i in hidden_states[-70:]])\n",
    "\n",
    "    return [\n",
    "        sum.detach().item(),\n",
    "        jacobian_score(hidden_state1),\n",
    "        jacobian_score(hidden_state2),\n",
    "        jacobian_score(hidden_state3),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62b70b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run scores on all architectures\n",
    "def run_scores(log_file, test_data, ntokens, criterion, writer, batch_num=0, seed_num=0):\n",
    "    log = json.load(open(log_file, \"r\"))\n",
    "    args = Namespace(**log)\n",
    "    args.cuda = True\n",
    "\n",
    "    global K_score\n",
    "    K_score = np.zeros((args.eval_batch_size, args.eval_batch_size))\n",
    "\n",
    "    global hidden_states\n",
    "    hidden_states = []\n",
    "    \n",
    "    # set seed for reproducability\n",
    "    random.seed(seed_num)\n",
    "    np.random.seed(seed_num)\n",
    "    torch.manual_seed(seed_num)\n",
    "\n",
    "    network = AWDRNNModel(\n",
    "        args.model,\n",
    "        ntokens,\n",
    "        args.emsize,\n",
    "        args.nhid,\n",
    "        args.nlayers,\n",
    "        args.dropout,\n",
    "        args.dropouth,\n",
    "        args.dropouti,\n",
    "        args.dropoute,\n",
    "        args.wdrop,\n",
    "        args.tied,\n",
    "        args.recepie,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        test_loss = args.test_losses[-1]\n",
    "    except:\n",
    "        test_loss = math.nan\n",
    "    print(\"-\" * 89)\n",
    "    try:\n",
    "        test_perplexity = math.exp(test_loss)\n",
    "    except:\n",
    "        test_perplexity = math.nan\n",
    "    test_bpw = test_loss / math.log(2)\n",
    "\n",
    "    # set device to run model on\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    network.to(device)\n",
    "    \n",
    "    param = sum(p.numel() for p in network.parameters())\n",
    "    train_param = sum(p.numel() for p in network.parameters() if p.requires_grad)\n",
    "\n",
    "    network.eval()\n",
    "    hidden = network.init_hidden(args.eval_batch_size)\n",
    "    data, target = get_batch(test_data, batch_num, args, evaluation=True)\n",
    "\n",
    "    # put network on device\n",
    "    network = network.to(device)\n",
    "\n",
    "    # inputs for binary codes score (K)\n",
    "    x2 = torch.clone(data)\n",
    "    x2 = x2.to(device)\n",
    "    data, target = data.to(device), target.to(device)\n",
    "\n",
    "    # get jacobian for old score\n",
    "    jacobs, labels, y, out = get_batch_jacobian(\n",
    "        network, data, target, device, hidden, args\n",
    "    )\n",
    "\n",
    "    # calculate K score\n",
    "    network(x2.to(device), hidden)\n",
    "    k_score = hooklogdet(K_score, target)\n",
    "    print(\"K Score: \" + str(k_score))\n",
    "\n",
    "    ### Old Jacobian Score Computation\n",
    "    j_score = jacobian_score(jacobs)\n",
    "    print(\"Jacobian Score: \" + str(j_score))\n",
    "\n",
    "    ### Jacobian Score Cosine\n",
    "    c_score = cosine_score(jacobs)\n",
    "    print(\"Jacobian Score Cosine: \" + str(c_score))\n",
    "    embedding_output = network.embeddings.detach().cpu().numpy()\n",
    "    variance = np.var(embedding_output, axis=1, keepdims=True)\n",
    "    noise = np.random.normal(size=embedding_output.shape)\n",
    "\n",
    "    ### Small Noise Score\n",
    "    sn_jacobs = noised_jacobian(\n",
    "        network,\n",
    "        1.0 / 10.0,\n",
    "        embedding_output,\n",
    "        variance,\n",
    "        noise,\n",
    "        target,\n",
    "        device,\n",
    "        hidden,\n",
    "        args,\n",
    "    )\n",
    "    sn_score = 1 - abs(j_score - jacobian_score(sn_jacobs))\n",
    "    print(\"Small Noise Score: \" + str(sn_score))\n",
    "\n",
    "    ### Small Noise Score Cosine\n",
    "    snc_score = 1 - abs(c_score - cosine_score(sn_jacobs))\n",
    "    print(\"Small Noise Score Cosine: \" + str(snc_score))\n",
    "\n",
    "    ### Large Noise Score\n",
    "    ln_jacobs = noised_jacobian(\n",
    "        network, 10, embedding_output, variance, noise, target, device, hidden, args\n",
    "    )\n",
    "    ln_score = 1 - abs(j_score - jacobian_score(ln_jacobs))\n",
    "    print(\"Large Noise Score: \" + str(ln_score))\n",
    "\n",
    "    ### Large Noise Score Cosine\n",
    "    lnc_score = 1 - abs(c_score - cosine_score(ln_jacobs))\n",
    "    print(\"Large Noise Score Cosine: \" + str(lnc_score))\n",
    "\n",
    "    ### More Noised Jacobian Score\n",
    "    vln_jacobs = noised_jacobian(\n",
    "        network, 100, embedding_output, variance, noise, target, device, hidden, args\n",
    "    )\n",
    "    mnj_score = (\n",
    "        j_score\n",
    "        * jacobian_score(sn_jacobs)\n",
    "        * jacobian_score(ln_jacobs)\n",
    "        * jacobian_score(vln_jacobs)\n",
    "    )\n",
    "    print(\"More Noised Jacobian Score: \" + str(mnj_score))\n",
    "\n",
    "    ### More Noised Jacobian Score Cosine\n",
    "    mnjc_score = (\n",
    "        c_score\n",
    "        * cosine_score(sn_jacobs)\n",
    "        * cosine_score(ln_jacobs)\n",
    "        * cosine_score(vln_jacobs)\n",
    "    )\n",
    "    print(\"More Noised Jacobian Score Cosine: \" + str(mnjc_score))\n",
    "\n",
    "    ### Synflow\n",
    "    synflow_score = synflow(network, data, target, device, hidden, args)\n",
    "    print(\"Synflow Score: \" + str(synflow_score))\n",
    "    \n",
    "    ### Hiden States Scores\n",
    "    hidden_states_scores = hidden_scores(hidden_states)\n",
    "    print(\"Hidden State Scores: \" + str(hidden_states_scores))\n",
    "\n",
    "    row = [\n",
    "        os.path.basename(log_file),\n",
    "        test_loss,\n",
    "        test_perplexity,\n",
    "        test_bpw,\n",
    "        k_score,\n",
    "        j_score,\n",
    "        c_score,\n",
    "        sn_score,\n",
    "        snc_score,\n",
    "        ln_score,\n",
    "        lnc_score,\n",
    "        mnj_score,\n",
    "        mnjc_score,\n",
    "        synflow_score,\n",
    "    ]\n",
    "\n",
    "    print(row)\n",
    "    writer.writerow(row + hidden_states_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34c7d5c-f2ac-4924-9f94-458e34aa2e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NAS-Bench-NLP with Penn Treebank dataset\n",
    "suffix = \"0025_2020-04-19_13-24-21_999981968\"\n",
    "log = json.load(\n",
    "    open(\"nas-bench-nlp-release/train_logs_multi_runs/log_stats_model_100\" + suffix + \".json\", \"r\")\n",
    ")\n",
    "args = Namespace(**log)\n",
    "args.cuda = True\n",
    "cuda = \"cuda:0\"\n",
    "\n",
    "if \"test_data\" not in globals():\n",
    "    corpus = nas_data.Corpus(args.data)\n",
    "    test_data = batchify(corpus.test, args.eval_batch_size, args, cuda)\n",
    "\n",
    "ntokens = len(corpus.dictionary)\n",
    "\n",
    "criterion = SplitCrossEntropyLoss(args.emsize, splits=[], verbose=False)\n",
    "\n",
    "K_score = np.zeros((args.eval_batch_size, args.eval_batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b7dd1b-0fc2-4368-bc2c-99b448a859b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation_models = [\"log_stats_model_1009532_2020-04-13_04-50-42_999940095.json\",\n",
    "\"log_stats_model_1003424_2020-04-22_06-45-47_999996414.json\",\n",
    "\"log_stats_model_1006809_2020-04-14_02-17-10_999987074.json\",\n",
    "\"log_stats_model_1004163_2020-04-21_07-14-54_999984871.json\",\n",
    "\"log_stats_model_1011260_2020-04-15_13-12-42_999971989.json\",\n",
    "\"log_stats_model_1002763_2020-04-22_17-21-45_999986705.json\",\n",
    "\"log_stats_model_1013157_2020-04-14_06-17-11_999980530.json\",\n",
    "\"log_stats_model_1002378_2020-04-16_11-37-11_999966726.json\",\n",
    "\"log_stats_model_1001317_2020-04-21_07-33-43_999989525.json\",\n",
    "\"log_stats_model_1013823_2020-04-16_09-58-08_999916458.json\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234959d7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run metrics on all model in benchmark\n",
    "with open(\"data/RNN_results.csv\", \"a\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    header = [\n",
    "        \"log name\",\n",
    "        \"loss\",\n",
    "        \"perplexity\",\n",
    "        \"bits per word\",\n",
    "        \"K Score\",\n",
    "        \"Jacobian Score\",\n",
    "        \"Jacobian Score Cosine\",\n",
    "        \"Small Noise Score\",\n",
    "        \"Small Noise Score Cosine\",\n",
    "        \"Large Noise Score\",\n",
    "        \"Large Noise Score Cosine\",\n",
    "        \"More Noised Jacobian Score\",\n",
    "        \"More Noised Jacobian Score Cosine\",\n",
    "        \"Synflow Score\",\n",
    "        \"Hidden Synflow Score\",\n",
    "        \"Hidden Layer 1 Covariance Score\",\n",
    "        \"Hidden Layer 2 Covariance Score\",\n",
    "        \"Hidden Layer 3 Covariance Score\",\n",
    "    ]\n",
    "    writer.writerow(header)\n",
    "    f.flush()\n",
    "\n",
    "    directory = \"nas-bench-nlp-release/train_logs_multi_runs\"\n",
    "    i = 1\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        log_file = os.path.join(directory, filename)\n",
    "        print(i)\n",
    "        try:\n",
    "            run_scores(log_file, test_data, ntokens, criterion, writer)\n",
    "        except:\n",
    "            row = [os.path.basename(log_file)] + [math.nan] * len(header)\n",
    "            writer.writerow(row)\n",
    "        f.flush()\n",
    "        i = i + 1\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb32e3d3-551b-47fe-b121-617e6a75d3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablation on different minibatches\n",
    "with open(\"data/RNN_batch_ablation.csv\", \"a\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    header = [\n",
    "        \"log name\",\n",
    "        \"loss\",\n",
    "        \"perplexity\",\n",
    "        \"bits per word\",\n",
    "        \"K Score\",\n",
    "        \"Jacobian Score\",\n",
    "        \"Jacobian Score Cosine\",\n",
    "        \"Small Noise Score\",\n",
    "        \"Small Noise Score Cosine\",\n",
    "        \"Large Noise Score\",\n",
    "        \"Large Noise Score Cosine\",\n",
    "        \"More Noised Jacobian Score\",\n",
    "        \"More Noised Jacobian Score Cosine\",\n",
    "        \"Synflow Score\",\n",
    "        \"Hidden Synflow Score\",\n",
    "        \"Hidden Layer 1 Covariance Score\",\n",
    "        \"Hidden Layer 2 Covariance Score\",\n",
    "        \"Hidden Layer 3 Covariance Score\",\n",
    "    ]\n",
    "    writer.writerow(header)\n",
    "    f.flush()\n",
    "\n",
    "    directory = \"nas-bench-nlp-release/train_logs_multi_runs\"\n",
    "    i = 1\n",
    "    \n",
    "    for filename in ablation_models:\n",
    "        log_file = os.path.join(directory, filename)\n",
    "        print(i)\n",
    "        for j in range(0, 10):\n",
    "            run_scores(log_file, test_data, ntokens, criterion, writer, batch_num=j)\n",
    "            f.flush()\n",
    "        i = i + 1\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22ff4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablation on different initializations\n",
    "with open(\"data/RNN_initialization_ablation.csv\", \"a\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    header = [\n",
    "        \"log name\",\n",
    "        \"loss\",\n",
    "        \"perplexity\",\n",
    "        \"bits per word\",\n",
    "        \"K Score\",\n",
    "        \"Jacobian Score\",\n",
    "        \"Jacobian Score Cosine\",\n",
    "        \"Small Noise Score\",\n",
    "        \"Small Noise Score Cosine\",\n",
    "        \"Large Noise Score\",\n",
    "        \"Large Noise Score Cosine\",\n",
    "        \"More Noised Jacobian Score\",\n",
    "        \"More Noised Jacobian Score Cosine\",\n",
    "        \"Synflow Score\",\n",
    "        \"Hidden Synflow Score\",\n",
    "        \"Hidden Layer 1 Covariance Score\",\n",
    "        \"Hidden Layer 2 Covariance Score\",\n",
    "        \"Hidden Layer 3 Covariance Score\",\n",
    "    ]\n",
    "    writer.writerow(header)\n",
    "    f.flush()\n",
    "\n",
    "    directory = \"nas-bench-nlp-release/train_logs_multi_runs\"\n",
    "    i = 1\n",
    "    \n",
    "    for filename in ablation_models:\n",
    "        log_file = os.path.join(directory, filename)\n",
    "        print(i)\n",
    "        for j in range(0, 10):\n",
    "            run_scores(log_file, test_data, ntokens, criterion, writer, seed_num=j)\n",
    "            f.flush()\n",
    "        i = i + 1\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
